{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1NHcfEa4nVVQzvf1g4PzrCVZp0xnm8VRu","authorship_tag":"ABX9TyPwTREDGAT4MFi+bbaRg9S9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EDS22-ptsF8E","executionInfo":{"status":"ok","timestamp":1675053994572,"user_tz":-480,"elapsed":954,"user":{"displayName":"Agd Tp","userId":"13309599981929937069"}},"outputId":"650c9f2a-41e2-4fff-f485-58c4b812837d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Jan 30 04:46:34 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   60C    P0    28W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import numpy as np\n","import joblib\n","from sklearn.preprocessing import MinMaxScaler\n","import torch\n","\n","\n","class PRAS_Dataset(Dataset):\n","\n","    def __init__(self,\n","                 input_len: int = 64,\n","                 output_len: int = 1,\n","                 train: bool = True,\n","                 transformer: bool = False,\n","                 train_test_split_ratio: float = 0.8,\n","                 file_path: str = '/content/drive/MyDrive/ADD/data/power.array'):\n","        super(PRAS_Dataset, self).__init__()\n","\n","        self.prsa_array = joblib.load(file_path)\n","\n","        self.train_array = self.prsa_array[:int(self.prsa_array.shape[0] * train_test_split_ratio)]\n","        self.test_array = self.prsa_array[int(self.prsa_array.shape[0] * train_test_split_ratio):]\n","\n","        # 注意训练集和测试集要分开做归一化，否则会造成数据泄露\n","        self.train_scaler = MinMaxScaler(feature_range=(-1, 1))\n","        self.test_scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","        self.tranformed_train_array = self.get_transform(self.train_scaler, self.train_array)\n","        self.tranformed_test_array = self.get_transform(self.test_scaler, self.test_array)\n","\n","        self.sequence_list = list()\n","        self.target_list = list()\n","        \n","        if not transformer:\n","            if train:\n","                for i in range(self.tranformed_train_array.shape[0] - input_len - output_len + 1):\n","                    self.sequence_list.append(self.tranformed_train_array[i: i+input_len])\n","                    self.target_list.append(self.tranformed_train_array[i+input_len: i+input_len+output_len])\n","            else:\n","                for i in range(self.tranformed_test_array.shape[0] - input_len - output_len + 1):\n","                    self.sequence_list.append(self.tranformed_test_array[i: i+input_len])\n","                    self.target_list.append(self.tranformed_test_array[i + input_len: i + input_len + output_len])\n","        else:\n","            if train:\n","                for i in range(self.tranformed_train_array.shape[0] - input_len - output_len + 1):\n","                    self.sequence_list.append(self.tranformed_train_array[i: i+input_len])\n","                    self.target_list.append(self.tranformed_train_array[i+output_len: i+input_len+output_len])\n","            else:\n","                for i in range(self.tranformed_test_array.shape[0] - input_len - output_len + 1):\n","                    self.sequence_list.append(self.tranformed_test_array[i: i+input_len])\n","                    self.target_list.append(self.tranformed_test_array[i + output_len: i + input_len + output_len])\n","\n","\n","    def __getitem__(self, idx):\n","        sequence = torch.tensor(self.sequence_list[idx], dtype=torch.float32)\n","        target = torch.tensor(self.target_list[idx], dtype=torch.float32)\n","\n","        return sequence, target\n","\n","    def __len__(self):\n","        return len(self.sequence_list)\n","\n","    def get_transform(self, scaler: MinMaxScaler, array: np.array):\n","        return scaler.fit_transform(array)\n","\n","    def get_inverse_transfoerm(self, scaler: MinMaxScaler, array: np.array):\n","        return scaler.inverse_transform(array)\n","\n","\n","# if __name__ == '__main__':\n","#     train_dataset = PRAS_Dataset(train=True)\n","#     test_dataset = PRAS_Dataset(train=False)\n"],"metadata":{"id":"HUYwZwirAAgF","executionInfo":{"status":"ok","timestamp":1675053998310,"user_tz":-480,"elapsed":3739,"user":{"displayName":"Agd Tp","userId":"13309599981929937069"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzT88j8D_Yat","executionInfo":{"status":"ok","timestamp":1675054011525,"user_tz":-480,"elapsed":13217,"user":{"displayName":"Agd Tp","userId":"13309599981929937069"}},"outputId":"552a53ac-9fdb-42ef-d21c-cc91e276b02c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","\n","class AttBLSTM(nn.Module):\n","    \n","    def __init__(self,\n","                 input_size: int = 15,\n","                 output_size: int = 15,\n","                 hidden_dim: int = 256,\n","                 lstm_dropout: float = 0,\n","                 linear_dropout=0.2):\n","        super(AttBLSTM, self).__init__()\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_dim = hidden_dim\n","        self.lstm_dropout = lstm_dropout\n","        self.linear_dropout = linear_dropout\n","\n","        self.blstm = nn.LSTM(self.input_size,\n","                             self.hidden_dim,\n","                             num_layers=2,\n","                             bidirectional=True,\n","                             batch_first=True,\n","                             dropout=self.lstm_dropout)\n","\n","        self.tanh = nn.Tanh()\n","        self.w = nn.Parameter(torch.Tensor(self.hidden_dim * 2, 1))\n","        torch.nn.init.kaiming_normal_(self.w)\n","        self.sigmoid = nn.Sigmoid()\n","        self.dropout = nn.Dropout(self.linear_dropout)\n","        self.linear = nn.Linear(self.hidden_dim * 2, self.output_size)\n","\n","    def forward(self, x):\n","        out, (h_n, c_n) = self.blstm(x)\n","\n","        M = self.tanh(out)\n","        alpha = F.softmax(torch.matmul(M, self.w), dim=1)\n","        out = out * alpha\n","        out = torch.sum(out, dim=1)\n","\n","        # out = self.tanh(out)\n","\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","\n","        return out\n","\n","    def name(self):\n","        return self.__class__.__name__\n"],"metadata":{"id":"p3v_NC2I-_Gy","executionInfo":{"status":"ok","timestamp":1675054011526,"user_tz":-480,"elapsed":4,"user":{"displayName":"Agd Tp","userId":"13309599981929937069"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","\n","class BLSTM(nn.Module):\n","\n","    def __init__(self,\n","                 input_size: int = 15,\n","                 output_size: int = 15,\n","                 hidden_dim: int = 256,\n","                 lstm_dropout: float = 0,\n","                 linear_dropout=0.1):\n","        super(BLSTM, self).__init__()\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_dim = hidden_dim\n","        self.lstm_dropout = lstm_dropout\n","        self.linear_dropout = linear_dropout\n","\n","        self.blstm = nn.LSTM(self.input_size,\n","                             self.hidden_dim,\n","                             num_layers=2,\n","                             bidirectional=True,\n","                             batch_first=True,\n","                             dropout=self.lstm_dropout)\n","\n","        self.dropout = nn.Dropout(self.linear_dropout)\n","        self.linear = nn.Linear(self.hidden_dim * 2, self.output_size)\n","\n","    def forward(self, x):\n","        out, (h_n, c_n) = self.blstm(x)\n","\n","        out = out[:, -1, :]\n","\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","\n","        return out\n","\n","    def name(self):\n","        return self.__class__.__name__\n"],"metadata":{"id":"UcmgXpRd_A3F","executionInfo":{"status":"ok","timestamp":1675054011526,"user_tz":-480,"elapsed":3,"user":{"displayName":"Agd Tp","userId":"13309599981929937069"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","\n","\n","class LSTM(nn.Module):\n","\n","    def __init__(self,\n","                 input_size: int = 15,\n","                 output_size: int = 15,\n","                 hidden_dim: int = 256,\n","                 lstm_dropout: float = 0,\n","                 linear_dropout=0.1):\n","        super(LSTM, self).__init__()\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.hidden_dim = hidden_dim\n","        self.lstm_dropout = lstm_dropout\n","        self.linear_dropout = linear_dropout\n","\n","        self.lstm = nn.LSTM(self.input_size,\n","                            self.hidden_dim,\n","                            num_layers=1,\n","                            bidirectional=False,\n","                            batch_first=True,\n","                            dropout=self.lstm_dropout)\n","\n","        self.dropout = nn.Dropout(self.linear_dropout)\n","        self.linear = nn.Linear(self.hidden_dim, self.output_size)\n","\n","    def forward(self, x):\n","        out, (h_n, c_n) = self.lstm(x)\n","\n","        out = out[:, -1, :]\n","\n","        out = self.dropout(out)\n","        out = self.linear(out)\n","\n","        return out\n","\n","    def name(self):\n","        return self.__class__.__name__\n"],"metadata":{"id":"SCrmZMZF_F-9","executionInfo":{"status":"ok","timestamp":1675054011526,"user_tz":-480,"elapsed":3,"user":{"displayName":"Agd Tp","userId":"13309599981929937069"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tsjJCLJV_Kc0","executionInfo":{"status":"ok","timestamp":1675054011526,"user_tz":-480,"elapsed":3,"user":{"displayName":"Agd Tp","userId":"13309599981929937069"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oO1OW6pz-kN-","outputId":"04fbd995-4ad7-47aa-fc1d-8e41a3225c70"},"outputs":[{"output_type":"stream","name":"stdout","text":["输入长度为16\n"]},{"output_type":"stream","name":"stderr","text":["\rModel Training:   0%|          | 0/4000 [00:00<?, ?it/s]"]}],"source":["import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.cuda import amp\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","import os\n","import joblib\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# from models import LSTM, AttBLSTM, Transformer, LinearTransformer, CNNTransformer, CNNTransformerHighway, MaskedLinearTransformer\n","# from utils import PRAS_Dataset\n","\n","'''\n","Typical Mixed Precision Training\n","'''\n","def clip_gradient(optimizer, grad_clip):\n","    \"\"\"\n","    Clips gradients computed during backpropagation to avoid explosion of gradients.\n","\n","    :param optimizer: optimizer with the gradients to be clipped\n","    :param grad_clip: clip value\n","    \"\"\"\n","    for group in optimizer.param_groups:\n","        for param in group[\"params\"]:\n","            if param.grad is not None:\n","                param.grad.data.clamp_(-grad_clip, grad_clip)\n","def train(model: nn.Module,\n","          train_loader: DataLoader,\n","          test_loader: DataLoader,\n","          learning_rate: float,\n","          epochs: int,\n","          input_len : int = 64,\n","          step_lr: bool = True,\n","          lr_change_step: int = 10,\n","          gamma: float = 0.99,\n","          device: str = 'cuda'):\n","    model.to(device)\n","    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_change_step, gamma=gamma)\n","    loss_caculate = nn.MSELoss().to(device)\n","    loss_ca1 = nn.L1Loss().to(device)\n","    scaler = amp.GradScaler(enabled=True)\n","\n","    train_loss_list = list()\n","    test_loss_list = list()\n","    test_loss = torch.Tensor([float('inf')])\n","\n","    weight_file_path = '/content/drive/MyDrive/ADD/alldata_weight_LSTM/'\n","\n","    if not os.path.exists(weight_file_path):\n","        os.mkdir(weight_file_path)\n","    break_flag = 0\n","    with tqdm(total=epochs, desc='Model Training') as pbar:\n","        for epoch in range(1, epochs + 1):\n","           \n","            train_total_loss = 0.\n","            test_total_loss = 0.\n","            train_total_loss1 = 0.\n","            test_total_loss1 = 0.\n","            '''\n","            Train model\n","            '''\n","            model.train()\n","            loss = None\n","\n","            for i in train_loader:\n","                sequence = i[0].to(device)\n","                target = i[1].squeeze(1).to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with amp.autocast(enabled=True):\n","                    ouputs = model(sequence)\n","                    loss = loss_caculate(ouputs, target[:, -1, :])\n","                    loss1 = loss_ca1(ouputs, target[:, -1, :])\n","                scaler.scale(loss).backward()\n","                clip_gradient(optimizer,10e5)\n","                scaler.step(optimizer)\n","                scaler.update()\n","\n","                train_total_loss += loss.item()\n","                train_total_loss1 += loss1.item()\n","                \n","\n","            if step_lr:\n","                scheduler.step()\n","\n","            '''\n","            Evaluate model\n","            '''\n","            model.eval()\n","            loss = None\n","\n","            with torch.no_grad():\n","                for i in test_loader:\n","                    sequence = i[0].to(device)\n","                    target = i[1].squeeze(1).to(device)\n","\n","                    ouputs = model(sequence)\n","                    loss = loss_caculate(ouputs, target[:, -1, :])\n","                    # loss = loss_caculate(ouputs, target)\n","                    loss1 = loss_ca1(ouputs, target[:, -1, :])\n","                    test_total_loss += loss.item()\n","                    test_total_loss1 += loss1.item()\n","                if test_total_loss <= test_loss:\n","                    for name in os.listdir(weight_file_path):\n","                        file = weight_file_path + '/' + name\n","                        # print(int(name.split('_')[-1].split('.')[0]))\n","                        try:\n","                          if int(name.split('_')[-1].split('.')[0]) == input_len:\n","                            os.remove(file)\n","                        except:\n","                          pass  \n","                    torch.save(model.state_dict(), weight_file_path + '/' + '{}_{}_{}_{}_{}.pt'.format(model.name(), epoch, test_total_loss, test_total_loss1, input_len))\n","                    test_loss = test_total_loss\n","                    final_name = weight_file_path + '/' + '{}_{}_{}_{}_{}.pt'.format(model.name(), epoch, test_total_loss, test_total_loss1, input_len)\n","                    break_flag = 0\n","                else:\n","                  break_flag += 1\n","            # train_loss_list.append(train_total_loss)\n","            # joblib.dump(train_loss_list, './logs/{}_train_loss_test_amp_0.list'.format(model.name()))\n","            # test_loss_list.append(test_total_loss)\n","            # joblib.dump(test_loss_list, './logs/{}_test_loss_test_amp_0.list'.format(model.name()))\n","\n","            tqdm.write('Epoch: {:5} | Train Loss: {:8}| Train MAE: {:8} | Test Loss: {:8}| Test MAE: {:8}  | LR: {:8}'.format(epoch, train_total_loss,train_total_loss1, \n","                                                                                          test_total_loss, test_total_loss1,\n","                                                                                            scheduler.get_last_lr()[0]))\n","\n","            pbar.update(1)\n","            if epoch>=350 and break_flag>=50:\n","              break \n","    return final_name\n","def test(model: nn.Module, \n","         test_dataset: PRAS_Dataset,\n","         device: str = 'cuda'):\n","    model.to(device)\n","    model.eval()\n","    loss_caculate = nn.MSELoss().to(device)\n","    loss_ca1 = nn.L1Loss().to(device)\n","    test_total_loss = 0.\n","    test_total_loss1 = 0.\n","    weight_file_path = '/content/drive/MyDrive/ADD/alldata_weight_LSTM/'\n","    predict_array_list = list()\n","    target_array_list = list()\n","\n","    for i in test_dataset:\n","        input_tensor = i[0].unsqueeze(0).to(device)\n","        target_tensor = i[1]\n","\n","        with torch.no_grad():\n","            output_tensor = model(input_tensor).squeeze(0).cpu()\n","\n","            loss = loss_caculate(output_tensor, target_tensor[-1])\n","            test_total_loss += loss.item()\n","\n","            loss1 = loss_ca1(output_tensor, target_tensor[-1])\n","            test_total_loss1 += loss1.item()\n","\n","            predict_array_list.append(output_tensor.numpy())\n","            target_array_list.append(target_tensor[-1].numpy())\n","\n","    input_array = test_dataset[0][0].cpu().numpy()\n","    predict_array = np.array(predict_array_list)\n","    target_array = np.array(target_array_list)\n","\n","    return input_array.T, predict_array.T, target_array.T, test_total_loss, test_total_loss1, len(test_dataset)\n","\n","\n","if __name__ == '__main__':\n","    file_path = '/content/drive/MyDrive/ADD/data/power.array'\n","\n","    batch_size = 1024\n","    do_train = True\n","    learning_rate = 0.0001\n","    epochs = 4000\n","\n","    do_test = False\n","    input_lens = [16, 32, 48, 64, 80, 96, 112, 128]\n","    # 定义模型时需要确定是单一变量预测还是多变量预测\n","    model = LSTM(input_size=8, output_size=8)\n","    for input_len in input_lens:\n","      print(f\"输入长度为{input_len}\")\n","      train_dataset = PRAS_Dataset(input_len=input_len, train=True, file_path=file_path, transformer=True)\n","      test_dataset  = PRAS_Dataset(input_len=input_len,train=False, file_path=file_path, transformer=True)\n","\n","      train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True, num_workers=2)\n","      test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, drop_last=True, num_workers=2)\n","\n","      start = time.time()\n","      final_name = \"\"\n","      if do_train:\n","          final_name = train(model=model, train_loader=train_loader, test_loader=test_loader, input_len=input_len, learning_rate=learning_rate, epochs=epochs)\n","\n","      end = time.time()\n","\n","      # print(end - start)\n","      \n","      if do_test:\n","          # model = BLSTM(input_size=15, output_size=15)\n","          model.load_state_dict(torch.load(final_name))\n","          i, p, t, loss_MSE, loss_MAE, length = test(model=model, test_dataset=test_dataset)\n","\n","          # for plot_feature_idx in range(0, 15):\n","          #     plt.figure(plot_feature_idx)\n","          #     plt.plot(range(0, i.shape[1]), i[plot_feature_idx], )\n","          #     plt.plot(range(i.shape[1], i.shape[1] + p.shape[1]), p[plot_feature_idx], )\n","          #     plt.plot(range(i.shape[1], i.shape[1] + p.shape[1]), t[plot_feature_idx], )\n","          \n","          # plt.show()\n","\n","\n","          print('MSE: {}'.format(loss_MSE))\n","          print('MAE: {}'.format(loss_MAE))"]},{"cell_type":"markdown","source":[],"metadata":{"id":"JMcblVjErCFf"}},{"cell_type":"code","source":[],"metadata":{"id":"oW67MPuRewE1","executionInfo":{"status":"aborted","timestamp":1675054029752,"user_tz":-480,"elapsed":4,"user":{"displayName":"Agd Tp","userId":"13309599981929937069"}}},"execution_count":null,"outputs":[]}]}